# GPU硬件基础

本章节聚焦GPU硬件与CUDA架构基础知识。建议与CUDA编程共同学习，在底层加深对GPU编程的理解，明确CUDA高性能计算和优化的目的、方向。

## 开场寄语

随着各种大模型的横空出世，AI不再停留在基本的应用，而是成为了新的科技生态，从底层到上层共同发展。越来越多的企业尝试构建自己的AI生态、训练专属的模型以应对复杂的业务、打破其他厂家的技术壁垒，这时AI的底层构建与部署就极为必要，催生出AI Infra的迅猛发展。

而基于底层硬件进行高性能计算，尽可能发挥硬件的价值、减少性能浪费而产生的额外成本，在AI生态中的重要性越来越明显，其应用在智能驾驶、工业自动化、量化等诸多领域。

基于计算硬件的高性能优化软件开发（如CUDA、昇腾CANN等），需要开发者具备扎实的硬件基础。这也是硬件、嵌入式开发工程师进入AI领域的好机会。

有一句话想送给大家：

> 如果有一列呈指数级加速的火车，你唯一要做的，就是**跳上去**。 一旦上车，**所有问题都可以在途中解决**。因此，试图预测一列每秒都在加速的火车会驶向何方，然后妄图在某个路口拦截它，这是徒劳的。
> ————黄仁勋最新专访：关于投资OpenAI、AI泡沫、ASIC的竞争

## 1 GPU硬件基础知识

首先让我们来看一个问题：GOU和CPU有什么区别，我们为什么要选用GPU作为高性能并行计算的硬件？

CPU和GPU的硬件架构不同，导致二者的拥有不同的性能和擅长的使用场景。

1. CPU的层级架构，包含计算单元、指令处理、L1缓存、L2缓存和内存；GPU则包含大量的计算单元，剩下的空间大部分留给了显存，其他单元很少。
2. 硬件架构的不同导致了不同的性能: CPU可用处理更复杂的工作流程，GPU适合处理简单且高强度、大规模的计算任务；CPU的算数逻辑单元和浮点单元少，但是CPU Core中的ALU和FPU能力更强，且CPU的缓存内存更多
3. 二者适合处理不同类型的工作负载。CPU通常适用于多任务和快速串行处理，GPU适用于大规模并行架构、高计算吞吐量的场景，浮点计算能力强大。

基于此，我们接下来详细介绍GPU的硬件结构。

### 1.1 GPU硬件架构

GPU架构演进：Fermi -> Kepler -> Volta -> Ampere -> Ada Lovelace(40系显卡)
GPU硬件架构（以Volta架构为例）：
GPU的核心：SM(多处理器)，负责线程调度与内存分配的核心，包含以下核心组件：

1. 计算单元：Cuda Cores(SP), 执行基本算术运算；Tensor Cores, 执行矩阵运算；FP64 Cores, 执行双精度浮点运算
2. 存储资源：Register File, 用于存储线程上下文；Shared Memory, Block内线程共享；L1 Cache/Texture Cache, 缓存子系统
3. 控制单元：Warp Scheduler, 管理warp调度；Dispatch Unit, 指令分发；其他特殊功能单元，执行特殊运算。

### 1.2 GPU内存层次与缓存

## 2 CUDA架构层详解

## 3 基于硬件的高性能优化

### 3.1 内存访问优化

### 3.2 并行性与资源限制

### 3.3 指令级优化

### 3.4 常用优化技术举例

#### 3.4.1 Roofline模型

#### 3.4.2 合并内存访问(Coalesced Memory Access)

#### 3.4.3 使用共享内存(Shared Memory)

#### 3.4.4 延迟隐藏(Latency Hiding)

#### 3.4.5 Warp级优化

#### 3.4.6 循环并行化(Loop Parallelization)
