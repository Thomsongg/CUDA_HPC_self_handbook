# CUDA 高频知识点

### 一、基础概念类
1. **GPU和CPU的区别是什么？**
    CPU和GPU的硬件架构不同，导致二者的拥有不同的性能和擅长的使用场景。
    (1) CPU的层级架构，包含计算单元、指令处理、L1缓存、L2缓存和内存；GPU则包含大量的计算单元，剩下的空间大部分留给了显存，其他单元很少。
    (2) 硬件架构的不同导致了不同的性能: CPU可用处理更复杂的工作流程；CPU的算数逻辑单元和浮点单元少，但是CPU Core中的ALU和FPU能力更强；CPU的缓存内存更多
    (3) 二者适合处理不同类型的工作负载。CPU通常适用于多任务和快速串行处理，GPU适用于大规模并行架构、高计算吞吐量的场景，浮点计算能力强大。
2. **什么是SIMT(单指令多线程)，和SIMD(单指令多数据)有什么区别？**
    官方解释：https://github.com/ForceInjection/AI-fundermentals/blob/main/gpu_architecture/gpu_characteristics.md
3. CUDA编程模型中的线程层次结构是什么？（网格、块、线程）
4. **解释一下warp的概念以及warp divergence。**
5. **什么是SM？什么是SP？**
详解：
    SM：流式多处理器，是线程处理的核心。每个SM上的CUDA Core以32个为一组排列；GPU中SM的数量取决于GPU架构。
    以kernel函数调用 func<<<M, N>>>为例:
    (1) GPU执行内核调用的SM数量取决于指定的线程块数量；且线程块不能被不同SM分割。
    (2) SM内采用SIMT管理线程，将每个线程块中N个线程通过调度程序分配给完整的Warp，在可用的32个Cuda Core组上运行，每个完整的Warp包含32个线程；线程块中剩余的Warp会分配给一组32个CUDA Core上运行。
    (3) 包含多级别内存，只能由此SM内的Cuda Core访问：寄存器、共享内存、常量缓存、L1缓存
    
1. **CUDA的内存模型有哪些？**（全局内存、共享内存、常量内存、纹理内存、寄存器等）
    
2. CUDA的同步方式有哪些？（__syncthreads, 内存栅栏，流同步等）


##### SM：GPU的核心
    SM架构组成 = {
    "计算单元": {
        "CUDA Cores (SP)": "执行基本算术运算",
        "Tensor Cores": "执行矩阵运算（现代架构）",
        "FP64 Cores": "双精度浮点运算"
    },
    "存储资源": {
        "Register File": "存储线程上下文",
        "Shared Memory": "Block内线程共享",
        "L1 Cache/Texture Cache": "缓存子系统"
    },
    "控制单元": {
        "Warp Scheduler": "管理warp调度",
        "Dispatch Unit": "指令分发",
        "特殊功能单元": "执行特殊运算"
    }
    }

> www
##### Warp Divergence
**优化思路：**
1. 尽量让同一warp内的线程走相同执行路径
2. 预处理数据，使相同条件的线程连续存储
3. 使用算术运算替代条件分支
4. 利用warp级别的投票和洗牌操作
5. 在算法设计阶段考虑数据局部性

**优化方法：**数据重排和预处理、使用谓词执行、线程束同步编程、算法重构、使用模板元编程

**warp divergence分析**：Nsight Compute检测
ncu --metrics smsp__warp_issue_stalled_long_scoreboard_per_warp_active.pct ./your_cuda_app
检查指标divergent_branches(发散分支数量)和branch_efficiency(分支效率)

### 二、内存管理类
1. 解释全局内存的合并访问（coalesced access）是什么？为什么重要？
2. 共享内存的bank conflict是什么？如何规避？
3. 常量内存和纹理内存的特点和适用场景？
4. 如何管理多个GPU设备？
5. 统一内存（Unified Memory）是什么？有什么优缺点？

### 三、性能优化类
1. 如何评估一个CUDA程序的性能？（计算吞吐量、内存带宽、占用率等）
2. 使用float4、float2等向量化加载有什么优势和缺点？
3. 如何选择block的大小？
4. 举例说明如何优化一个CUDA内核。
5. 什么是occupancy？如何计算？

### 四、编程模型和工具
1. CUDA Streams的作用是什么？如何用Streams实现并发？
2. 动态并行（Dynamic Parallelism）是什么？
3. 如何使用Nsight Systems和Nsight Compute进行性能分析？
4. CUDA事件（Events）的作用？

### 五、CUDA流处理(Streaming)
1. CUDA流处理是什么机制？ -> 并发机制，同一流内顺序执行，不同流间并发执行且独立；
2. 流处理适用于哪些场景？ -> 内核执行与数据传输重叠、多个独立内核并发执行、流水线处理、多GPU编程，即单核函数多批次处理、多核函数多批次(流事件cudaStreamWaitEvent)
3. 流处理的基本过程有哪些？ -> 分配内存、创建流、计算各流的数据量、并发处理(以单内核多流为例：异步内存传输-cudaMemcpyAsync指定streams[i]、启动核函数)、等待所有流完成(cudaStreamSynchronize)

### 六、编程实战
1. element-wise算子(向量加法、sigmoid激活函数)
2. 归约(reduction)
3. 矩阵乘法(GEMM)
4. 高频算子(softmax)
5. 前缀和(scan)
6. 卷积算子

### 七、使用 Nsight Compute 估算程序性能
1. GPU Speed Of Light Throughput（GPU吞吐量）
   

> (1) Memory Throughput：内存吞吐量，可根据Roofline模型判断，过高则程序存在计算强度瓶颈，过低则可能存在内存访问瓶颈
(2) DRAM Throughput：DRAM吞吐量，过低，说明大部分数据在缓存中命中，或者内存访问模式不佳
(3) Compute (SM) Throughput：SM的计算吞吐量，体现计算资源利用率的高低
(4) L1/TEX Cache Throughput：L1/TEX缓存吞吐量
(5) L2 Cache Throughput

1. Launch Statistics（启动统计）
2. Occupancy（占用率）
    
```
(1) 如何计算理论占用率？
    理论占用率 = 实际可运行线程数 / 每个SM最大线程数
    实际可运行线程数 = 限制块数 * 每块的线程数 = min(寄存器限制， 共享内存限制， 线程块限制， Warp限制) * Block Size
    寄存器限制块数 = 每个SM寄存器总数(最新架构为65536) / 每个块寄存器总数(块内线程数 * 每个线程的寄存器数)
    共享内存限制块数 = 每个SM的共享内存 / 每个块共享内存
    线程块限制 = 每个SM内最大线程块数(最新架构为32)
    Warp限制的块数 = 每个SM最大Warp数(最新架构为64) / 每个块内Warps数
```

```
(2) 实际占用率远低于理论值，会有什么问题，可能有什么原因？如何提升实际占用率？
提升实际占用率：增加可用Warp(块内线程数)、减少共享内存使用
```

3. Warp Divergence（线程束分化）判断
4. 潜在Bank Conflict分析
5. Partial Wave问题
    **原理：**Partial Wave 指的是在GPU执行kernel时，网格中的线程块不能完全填满所有SM的最后一波执行。
   ** 产生原因：**
        
```
(1) 网格大小不是Wave的整数倍

(2) 资源限制导致的低占用率：
    共享内存限制：每个块8.45KB → 每个SM最多10个块
    寄存器限制：每个线程49个寄存器
    线程块大小：64线程/块

(3)问题规模与硬件不匹配：
    固定的矩阵尺寸(1024×512)
    固定的BLOCK_SIZE(32)和SUB_TILE_SIZE(4)
```
    
**方法：**
(1) 调整网格大小为Wave的整数倍：调整BLOCK_SIZE或SUB_TILE_SIZE
(2) 使用动态并行或调整问题划分：调整网格大小，或调整每个线程的计算强度
(3) 资源优化以减少限制：减少共享内存使用(增加每个SM运行的线程块数)，或优化寄存器使用

#### 常用的高性能计算优化技术
##### 1 Roofline模型
原理：通过算术强度和硬件极限来识别性能瓶颈
算术强度 x 峰值带宽 与 峰值算力相比，即判断计算强度的高低
小于，则为内存瓶颈；大于，则为计算瓶颈

##### 2 合并内存访问 (Coalesced Memory Access)
目的：让同一warp的32个线程访问连续的内存地址
实现：
1. 数据布局优化：AOS 替代 SOA
    举例：
    (1) 内存不连续的情况，性能较差：
```
struct Particles
{
    float x,y,z;
    float vx,vy,vz;
    float mass;
}
// 相邻线程访问不连续的内存
// thread0 访问 particles[0].x
// thread1 访问 particles[1].x
// 相邻线程间隔sizeof(Particles)，即 40个字节
particles[tid].x += particles[tid].vx * dt;
```

(2) 修改结构体后，合并内存访问：
```
struct Particles
{
    float *x, *y, *z;
    float *vx, *vy, *vz;
    float mass;
}
// 使用单个结构体对象，其成员按连续的内存空间存储
// 相邻线程访问连续的内存
// thread0 访问 particles.x[0]
// thread1 访问 particles.x[1]
particles.x[tid] += particles.vx[tid] * dt;

```
2. 访问模式调整：使用向量化等方法，合并访问
3. 共享内存中转

##### 3 使用共享内存(Shared Memory)
原理：使用线程块内使用共享内存进行访问，替代全局内存；通过数据分块实现数据重用，减少全局内存访问，提升读写速度；注意避免bank conflict

##### 4 延迟隐藏(Latency Hiding)
问题：部分线程在等待数据传输时处于空闲状态，造成延迟
原理：使用其他就绪的线程进行计算，掩盖内存访问延迟
1. 增加Occupancy(占用率)：更多的并发warp；每个线程处理多个元素、增加计算强度(如grid-stride)
2. 指令级并行：单个warp内安排独立操作(如在等待数据加载时执行独立计算)
3. 预取技术：提前加载待处理的数据

##### 5 Warp级优化

##### 6 循环并行化(Loop Parallelization)
1. 循环展开：
    目的：降低循环控制开销(如条件判断、循环结束、变量递增)，增加指令级并行
    方法：手动展开、编译器展开(#pragma unroll)
    利弊：循环次数或临时变量过多时，展开后会显著增加寄存器压力
2. 循环分块
    目的：提高缓存命中率，提升内存带宽和数据传输速率
    原理：将大循环分解为适合缓存的小块
    方法：共享内存、寄存器，如矩阵乘法、卷积等数据重用场景

3. 循环合并
    目的：减少内存访问事务
    方法：合并多个小循环为单个循环 

### 附录
##### 1 Nvidia GPU的内存类型
以下来源于CUDA官方文档：
    
    (1) 寄存器文件 - 表示直接进入 CUDA 核心的内存区域。相应地，它被组织成 32 个bank，与一个 warp 中的 32 个线程相匹配。将寄存器文件视为一个由 4 字节元素组成的大矩阵，有很多行和 32 列。一个 warp 在完整的行上操作；在给定的行中，每个线程（CUDA 核心）在不同的列（bank）上操作。

    (2) L1 缓存 - 指的是通常的片上存储位置，提供对最近从主内存（RAM）读取或写入的数据的快速访问。此外，当活动数据量超过 SM 的寄存器文件可以容纳的量时，L1 还充当溢出区域，这种情况被称为“寄存器溢出”。在 L1 中，缓存行和溢出的寄存器被组织成银行，就像寄存器文件一样。

    (3) 共享内存 - 是一个物理上位于与 L1 缓存相同的内存中的内存区域，但它与 L1 不同，因为它的所有数据都可以被一个线程块中的任何线程访问。这允许线程相互通信和共享数据。占用它的变量必须由应用程序显式声明。应用程序还可以设置 L1 和共享内存之间的分界线。

    (4) 常量缓存 - 是专门用于全局内存中声明为只读常量的变量的特殊缓存。这些变量可以被线程块中的任何线程读取。这些缓存的主要和最佳用途是将单个常量值广播到 warp 中的所有线程。

    (5) L2 缓存 - 是一个进一步的片上缓存，用于保留在 SM 和主内存之间来回传输的数据的副本。像 L1 一样，L2 缓存旨在加速后续的重新加载。但与 L1 缓存不同，只有一个 L2 被所有 SM 共享。L2 缓存也位于通过 PCIe 或 NVLink 在设备上或下移动的数据的路径上。

    (6) 全局内存 - 代表设备的大部分主内存，相当于基于 CPU 的处理器中的 RAM。出于性能原因，Tesla V100 拥有特殊的 HBM2 高带宽内存，而 Quadro RTX 5000 拥有快速的 GDDR6 图形内存。

    (7) 局部内存 - 对应于分配给每个 SM 的主内存的特定映射区域。每当“寄存器溢出”溢出特定 SM 的 L1 缓存时，超出的数据会被进一步卸载到 L2，然后是“局部内存”。重新加载溢出寄存器的性能惩罚随着必须遍历的每个内存级别而变得更加严重。

    (8) 纹理和常量内存 - 是被视为设备只读的内存区域。当它们被传输到 SM 时，具有“纹理”或“常量”声明的变量可以被线程块中的任何线程读取，就像共享内存一样。纹理内存被缓存在 L1 中，而常量内存被缓存在常量缓存中。

##### 2 GPU显存和CPU内存有什么区别？
核心区别：
    (1) 性能不同：CPU内存低延迟、高容量，采用DDR4/DDR5；GPU内存超高贷款、小容量，通常采用GDDR6(Quadro RTX 5000)/HBM2(Tesla架构)
    (2) 两者的内存在物理上隔离，无法通过主机/设备代码直接控制GPU/CPU特定内存

##### 3 有哪些方法可用降低GPU内存延迟高的问题？
    (1) 硬件：SM的共享内存、L1缓存，避免与DRAM直接交互的延迟
    (2) 软件：采用延迟隐藏技术，GPU代码侧启动大量的线程，使部分warp等待数据停滞时，其他warp可用从指令缓冲区获取下一个指令继续执行
    (3) 主机内存：主机与设备通过PCIe连接，传输速率小于NVLink，会限制GPU的性能。理想情况下，需要GPU的处理的数据，尽可能保留在GPU中进行运算

##### 4 Volta架构及其SM有什么特点
![](./images/image.png)
    Volta架构的SM：
    (1) 包含80个SM
    (2) 包含4个处理模块，每个模块可用通过Warp调度器和L0指令缓存，来调度CUDA Cores；每个时钟周期内，最多可处理2个FP32/INT32的warp，或1个FP64的warp
    (3) 每个Volta SM的处理块都包含2个Tensor Cores，用来保存输入数据和神经网络层之间连接的优化权重，快速进行矩阵乘法，，提高卷积神经网络(CNN)的训练和推理速度；每个Tensor Core用于对4x4矩阵做矩阵乘法 D = A X B + C
    (4) 部分GPU设备支持ECC校验
    (5) 支持统一内存：内存分配和数据传输，由CUDA和硬件协同，将请求的内存页迁移到访问处理器的物理内存，主机和设备均可
    (6) 会在CPU端启动多个内存传输流

##### 5 专业图形卡和计算加速卡在硬件上有什么区别？
以Tesla V100(Volta架构计算卡)和 Quadro RTX 5000(Turing架构图形卡)为例：
    (1) 计算卡的浮点(单双)计算能力强，SM甚至包含FP64的计算单元，适用于极高数值精度的领域，计算单元数量更多
    (2) 计算卡的数据吞吐量高，采用HBM2高带宽、低功耗的内存，而图形卡采用GDDR6的内存
    (3) Tensor Cores不同，计算卡转为AI优化，用于执行混合精度的矩阵乘法加法运算；图形卡主要用于AI降噪和DLSS等图形特性
    (4) 计算卡没有RT Core，无法处理实时光线追踪
    (5) V100等计算卡标配ECC功能

##### 6 如何理解 thread-warp-block-grid的结构
核心：低一级的数量超标时，由高一级统一管理，将低一级的单元分组，共享内存并同步执行、均衡负载

##### 7 GPGPU(以H100为代表)和NPU(昇腾910B为代表)有哪些区别，适用于什么场景？
GPGPU（以 H100 为代表）的核心优势：

1. 生态成熟度高：CUDA 生态完善，开发工具链丰富，社区支持强大
2. 通用性强：支持多种计算模式，适应性广，调试便利
3. 大模型训练优势明显：在复杂模型训练中性能领先 15-20%
4. 精度支持全面：从 FP64 到 INT1 全精度支持，满足不同场景需求

NPU（以昇腾 910B 为代表）的核心优势：
1. 推理效率突出：在大规模推理场景中能效比优势显著
2. 成本效益明显：采购成本低 30-40%，运营成本低 15-25%
3. 量化友好：硬件原生支持 INT8/INT4 量化，推理性能优异
4. 边缘部署适配性强：功耗控制和集成度优势明显

##### 8 NVLink是什么，与PCIe链路有什么区别，多卡环境下最大的数据链路带宽瓶颈是哪里？


##### 9 大模型涉及的精度有哪些，简单介绍下
有FP64、FP(INT)32、FP(INT)16、FP(INT)8、FP4等

##### 10 分布式存储是什么，有哪些关键技术
**定位：**是AI基础设施的核心组件，为大规模训练和推理提供高性能、高可靠性的数据存储解决方案；主要应对海量训练数据集、频繁的模型检查点保存、多节点并发访问等挑战。

**关键技术包含：**
1. 高吞吐量数据访问：支持多 GPU 节点并发读取训练数据，提供聚合带宽达到数十 GB/s 的性能表现
2. 元数据管理优化：针对深度学习场景的小文件密集访问模式，优化元数据缓存和索引机制
3. 数据一致性保证：在分布式训练过程中确保检查点数据的强一致性，支持故障恢复和断点续训
4. 存储层次化：结合 NVMe SSD、HDD 和对象存储，实现冷热数据分层管理和成本优化
5. 网络优化：利用 RDMA、NVLink 等高速网络技术，减少存储 I/O 延迟对训练性能的影响

常用分布式文件系统：JuiceFS\3FS

##### 11 AI基础设施中，常用的高性能网络与通信技术有哪些
1. InfiniBand网络技术
2. RDMA远程直接内存访问
3. NCCL分布式通信
